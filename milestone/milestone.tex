\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final]{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Milestone: Classifying Phytolith with 3D Features}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  James Noeckel, Chenglong Wang\\
  University of Washington
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  We plan to study the classification problem of phytoliths using a stack of images generated by scanning the phytoliths using a confocal microscopy. In this report, we present discoveries made from data analysis and the result from our baseline experiment. Our initial result shows that due to the lack of high volume training data, the baseline convolution network model achieves 34\% of classification accuracy on the validation dataset, which under performs a baseline linear model with logistic regression achieving 43\% accuracy.
\end{abstract}

\section{Introduction}

Many real world object are presented in forms of 2D stacks, e.g., phytoliths structures or human body scanning result. Instead of directly flattening 2D image stacks into a single image before feeding into a neural network, prior work shows that incorporating 3D structures in image embedding can significantly improve the classifier performance. We plan to study how to effectively extract 3D features from such image stacks to improve classifier accuracy as well as training efficiency.

So far, we conducted of a set of baseline experiments without incorporating 3D features. Concretely, we experimented with a linear classifier and a convolution neural network model on the dataset. We set two training goals for each model: (1) predicting only the family of the phytolith and (2) predicting both the family and the sub-family. The former task is a binary classification problem on the dataset and the latter is a 7 class classification problem. Our experiment shows that both linear and CNN model performs similarly on the binary classification (with $\sim$80\% validation accuracy), but in the second task, the linear model outperforms the CNN model by 5\% (45\% v.s. 40\%) validation accuracy. Our future plan is to incorporate 3D information together with better modeling to improve the model performance

\section{Dataset Description}

The raw data that we are considering for learning phytolith species consists of image stacks from confocal micrographs of individual specimen. The images have varying resolution, and the number of slices also varies. Considered as a whole, these image stacks provide a three-dimensional representation of the organisms. However, because each pixel is the result of focusing a cone of light at a particular point in the volume, reconstructing the actual shape from the content of the 2D image slices is nontrivial.
The labels are hierarchical, consisting of subfamilies, tribes, and finally the species. All in all, there are 889 data points with around 20 per species. Although the data is incredibly high dimensional (in the tens of millions of dimensions) due to being volumetric, there are relatively few examples from which to learn. It is for this reason that we plan to identify and target high-level features that might be especially helpful to the classification problem, as described in our future plans.


\section{Baseline Models \& Result}

Our baseline models include a linear classifier and a CNN model. In this section, we use $\mathbf{X}$ and $\mathbf{y}$ to refer to our training input and output, where each ${x}_i^T\in \mathbf{X}$ is a vector representing a image and each ${y_i}\in \mathbf{y}$ is the label of that image. Depending on the training objective, the label $y_i$ is an integer either between 0-7 (for predicting both family and sub-family) or 0-1 (for family only). We use $\mathbf{p}$ to represent predictions made by our classifier. Our model is implemented using Tensorflow v1.4.

Each raw image stack (shaped 256 * 512 * 512) is compressed into one single image (512 * 512) by mean-pooling, and then uniformly resized into 128 * 128 sized images.

\paragraph{Linear Model}

Our linear model predicts the label using linear transformation, as follows

$$\mathit{logits} = \mathbf{X} \mathbf{W} + \mathbf{b}$$
$$\mathbf{p} = \arg\max(\mathit{logits})$$


The training objective is to minimize the cross entropy loss (with L1 regularization):

$$\mathit{loss} = -\sum\limits_{i} y_i\log(\mathit{softmax}(\mathit{logits})) + \lambda \|\mathbf{W}\|_1$$

\paragraph{CNN Model}

Our CNN model is a two layer convolution network. The first layer consists of 32 filters and the second with 64 filters, both using max-pooling with stride 2 and kernel size 2.

The output from the second layers is implemented with dropout 0.4 to avoid over-fitting due to the small amount of data, and then the result is sent to a fully connected layer for decoding, which is the same as the one in the linear model.

\medskip

Both of our models are trained using Adagrad optimizer with learning rate 0.1 and trained 40 epoches.
Our result shows that the CNN model achieves 80.5\% validation accuracy on family classification and 34.7\% 7-class classification, while the linear model achieves 89.4\% in the 2-classes task and 57.1\% in the 7-class task.


\section{Follow-up Plans}

\paragraph{Transfer learning} One problem of the dataset is that we have a very small set of the data (889 image stacks), and CNN model does not benefit from its expressiveness. Our plan is to include a synthetic dataset to initialize our model together with phytolith datasets from other sources (Missouri Phytolith Database~\footnote{http://phytolith.missouri.edu/}, and Phytcore dataset~\footnote{http://www.phytcore.org/phytolith/}).

\paragraph{Utilizing 3D information from image stacks}

In order to use common image recognition techniques (such as convolutional neural networks) to analyze the data, some simple methods to reduce the dimensionality of the data exist, such as taking the mean, minimum, or maximum along the depth axis. However, this discards information. 

We intend to make better use of the 3D information present in the raw data by developing a neural network architecture specifically for classifying 3D voxels. Due to the relative scarcity of data for a deep learning approach, we have decided to augment our dataset with synthetic data so that our model can better learn the relationship between shapes and their image stack representations. For example, we might take a deformable part model approach by learning to recognize certain primitives shapes that comprise the 3D structures. To this end, we have developed a framework for generating synthetic confocal microscope data from basic 3D shapes, creating image stacks with the objects oriented randomly. It is also important to incorporate the optical artifacts inherent to the measurement technique, which we have done by convolving the sharp 3D volumes with a depth-axis aligned conic filter. Topics of future investigation will therefore involve choosing a set of features with reasonable coverage, finding a neural network architecture that is suited to the associated recognition tasks, and assessing the performance of a classification method using these features.

%[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms
%for connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and
%T.K.\ Leen (eds.), {\it Advances in Neural Information Processing
%  Systems 7}, pp.\ 609--616. Cambridge, MA: MIT Press.

%[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS:
%  Exploring Realistic Neural Models with the GEneral NEural SImulation
%  System.}  New York: TELOS/Springer--Verlag.

%[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of
%learning and recall at excitatory recurrent synapses and cholinergic
%modulation in rat hippocampal region CA3. {\it Journal of
%  Neuroscience} {\bf 15}(7):5249-5262.

\end{document}