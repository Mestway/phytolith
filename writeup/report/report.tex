\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final]{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Nerual Phytolith Classifier}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  James Noeckel, Chenglong Wang\\
  University of Washington
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  We study the classification problem of phytoliths using stacks of images generated by scanning confocal microscopy. In this report, we present our methods of data preprocessing and dataset augmentation, as well as the accuracy of various models when used as classifiers. Due to the small volume of training data, we avoid using complex models, comparing the accuracy of a linear classifier, a neural network with one hidden fully-connected layer, a neural network with one hidden convolutional layer, and a neural network with two convolutional layers and three fully-connected layers. Additionally, we supervise the classifier on all three labels (sub-family, tribe, and genus) during training, and additional training data is generated from existing data subjected to random transformations. The results are about 60\% accuracy on genus, 77\% accuracy on tribe, and 95\% accuracy on sub-family, with the single convolutional network performing best on genus and the two-convolutional and three-fully-connected network performing best on sub-family and tribe. The small amount of data makes interpreting test performance difficult, and our tree-based supervision or dataset augmentation do not have consistent effects on accuracy.
\end{abstract}

\section{Introduction}

While there exists a large amount of practical image classifiers, the problem for classifying phytolith images remains challenging

Many real world objects are represented in the form of 3D volumes, e.g., phytoliths structures or medical scans. Instead of directly flattening 2D image stacks into a single image before feeding into a neural network, prior work shows that incorporating 3D structures in image embedding can significantly improve classification results. We plan to study how to effectively extract 3D features from such image stacks to improve classifier accuracy as well as training efficiency.

So far, we conducted a set of baseline experiments without incorporating 3D features. Concretely, we experimented with a linear classifier and a convolutional neural network model on the dataset. We set two training goals for each model: (1) predicting only the sub-family of the phytolith and (2) predicting both the sub-family and the tribe. The former task is a binary classification problem on the dataset and the latter is a 7 class multimodal classification problem. Our experiment shows that both linear and CNN models perform similarly on the binary classification into sub-families (with $\sim$80\% validation accuracy), but in the second task, the linear model outperforms the CNN model by 5\% (45\% v.s. 40\%) validation accuracy. Our future plan is to incorporate 3D information together with a more intelligent model to improve the performance (see future plans).

\section{Dataset Description}

The raw data that we are considering for learning phytolith species consists of image stacks from confocal micrographs of individual specimen. The images have varying resolution, and the number of slices also varies. Considered as a whole, these image stacks provide a three-dimensional representation of the organisms. However, because each pixel is the result of focusing a cone of light at a particular point in the volume, reconstructing the actual shape from the content of the 2D image slices is nontrivial.
The labels are hierarchical, consisting of subfamilies, tribes, and finally the species. All in all, there are 889 data points with around 20 per species. Although the data is incredibly high dimensional (in the tens of millions of dimensions) due to being volumetric, there are relatively few examples from which to learn. It is for this reason that we plan to identify and target high-level features that might be especially helpful to the classification problem, as described in our future plans.


\section{Baseline Models \& Result}

Our baseline models include a linear classifier and a CNN model. In this section, we use $\mathbf{X}$ and $\mathbf{y}$ to refer to our training input and output, where each ${x}_i^T\in \mathbf{X}$ is a vector representing a image and each ${y_i}\in \mathbf{y}$ is the label of that image. Depending on the training objective, the label $y_i$ is an integer either between 0-7 (for predicting both family and sub-family) or 0-1 (for family only). We use $\mathbf{p}$ to represent predictions made by our classifier. Our model is implemented using Tensorflow v1.4.

Each raw image stack (shaped 256 * 512 * 512) is compressed into one single image (512 * 512) by mean-pooling, and then uniformly resized into 128 * 128 sized images.

\paragraph{Linear Model}

Our linear model predicts the label using linear transformation, as follows

$$\mathit{logits} = \mathbf{X} \mathbf{W} + \mathbf{b}$$
$$\mathbf{p} = \arg\max(\mathit{logits})$$


The training objective is to minimize the cross entropy loss (with L1 regularization):

$$\mathit{loss} = -\sum\limits_{i} y_i\log(\mathit{softmax}(\mathit{logits})) + \lambda \|\mathbf{W}\|_1$$

\paragraph{CNN Model}

Our CNN model is a two layer convolution network. The first layer consists of 32 filters and the second with 64 filters, both using max-pooling with stride 2 and kernel size 2.

The output from the second layers is implemented with dropout 0.4 to avoid over-fitting due to the small amount of data, and then the result is sent to a fully connected layer for decoding, which is the same as the one in the linear model.

\medskip

Both of our models are trained using Adagrad optimizer with learning rate 0.1 and trained 40 epoches.
Our result shows that the CNN model achieves 80.5\% validation accuracy on family classification and 34.7\% 7-class classification, while the linear model achieves 89.4\% in the 2-classes task and 57.1\% in the 7-class task.


\section{Follow-up Plans}

\paragraph{Transfer learning} One problem of the dataset is that we have a very small set of the data (889 image stacks), and CNN model does not benefit from its expressiveness. Our plan is to include a synthetic dataset to initialize our model together with phytolith datasets from other sources (Missouri Phytolith Database~\footnote{http://phytolith.missouri.edu/}, and Phytcore dataset~\footnote{http://www.phytcore.org/phytolith/}).

\paragraph{Utilizing 3D information from image stacks}

In order to use common image recognition techniques (such as convolutional neural networks) to analyze the data, some simple methods to reduce the dimensionality of the data exist, such as taking the mean, minimum, or maximum along the depth axis. However, this discards information. 

We intend to make better use of the 3D information present in the raw data by developing a neural network architecture specifically for classifying 3D voxels\cite{DBLP:conf/iros/MaturanaS15,brock2016generative}. Due to the relative scarcity of data for a deep learning approach, we have decided to augment our dataset with synthetic data so that our model can better learn the relationship between shapes and their image stack representations. For example, we might take a deformable part model approach by learning to recognize certain primitives shapes that comprise the 3D structures. To this end, we have developed a framework for generating synthetic confocal microscope data from basic 3D shapes, creating image stacks with the objects oriented randomly. It is also important to incorporate the optical artifacts inherent to the measurement technique, which we have done by convolving the sharp 3D volumes with a depth-axis aligned conic filter. Topics of future investigation will therefore involve choosing a set of features with reasonable coverage, finding a neural network architecture that is suited to the associated recognition tasks, and assessing the performance of a classification method using these features.

\bibliography{milestone}
\bibliographystyle{plain}
\end{document}